{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fastervit\n",
      "  Downloading fastervit-0.9.8-py3-none-any.whl.metadata (9.6 kB)\n",
      "Requirement already satisfied: timm>=0.6.12 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from fastervit) (0.9.16)\n",
      "Requirement already satisfied: torchvision in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from fastervit) (0.17.0+cu121)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from fastervit) (6.0.1)\n",
      "Requirement already satisfied: torch in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from timm>=0.6.12->fastervit) (2.2.0+cu121)\n",
      "Requirement already satisfied: huggingface_hub in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from timm>=0.6.12->fastervit) (0.20.3)\n",
      "Requirement already satisfied: safetensors in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from timm>=0.6.12->fastervit) (0.4.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torchvision->fastervit) (1.26.4)\n",
      "Requirement already satisfied: requests in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torchvision->fastervit) (2.28.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torchvision->fastervit) (10.2.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch->timm>=0.6.12->fastervit) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch->timm>=0.6.12->fastervit) (4.8.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch->timm>=0.6.12->fastervit) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch->timm>=0.6.12->fastervit) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch->timm>=0.6.12->fastervit) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch->timm>=0.6.12->fastervit) (2024.2.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface_hub->timm>=0.6.12->fastervit) (4.66.2)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\admin\\appdata\\roaming\\python\\python310\\site-packages (from huggingface_hub->timm>=0.6.12->fastervit) (23.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->torchvision->fastervit) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->torchvision->fastervit) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->torchvision->fastervit) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->torchvision->fastervit) (2022.12.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\appdata\\roaming\\python\\python310\\site-packages (from tqdm>=4.42.1->huggingface_hub->timm>=0.6.12->fastervit) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->torch->timm>=0.6.12->fastervit) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy->torch->timm>=0.6.12->fastervit) (1.3.0)\n",
      "Downloading fastervit-0.9.8-py3-none-any.whl (165 kB)\n",
      "   ---------------------------------------- 0.0/165.7 kB ? eta -:--:--\n",
      "   -------------- ------------------------- 61.4/165.7 kB 1.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 165.7/165.7 kB 2.0 MB/s eta 0:00:00\n",
      "Installing collected packages: fastervit\n",
      "Successfully installed fastervit-0.9.8\n"
     ]
    }
   ],
   "source": [
    "!pip install fastervit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importance\n",
    "\n",
    "Before using FasterVit, if this error shows up\n",
    "\n",
    "`cannot import name '_update_default_kwargs' from 'timm.models._builder'`\n",
    "\n",
    "Go to original `faster_vit.py` and `faster_vit_any_res.py` and change all the function from `_update_default_kwargs` to `_update_default_model_kwargs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastervit import create_model\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.transforms import Compose, ToTensor, Lambda, Resize, Normalize\n",
    "from PIL import Image, ImageDraw\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import accuracy_score,f1_score\n",
    "\n",
    "from tqdm import tqdm\n",
    "from transformers import CLIPTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIRECTROY = 'data'\n",
    "MODEL_PATH = 'models'\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 100\n",
    "LR = 0.0001\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(f'{DIRECTROY}/reduced_train.csv') \n",
    "df_test = pd.read_csv(f'{DIRECTROY}/reduced_test.csv') \n",
    "num_classes = len(df_train['newid'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_transforms = Compose([\n",
    "    Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    ToTensor(), \n",
    "    Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df, transforms, directory):\n",
    "        self.tokenizer =  CLIPTokenizerFast.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
    "        self.df = df\n",
    "        self.transforms = transforms\n",
    "        self.directory = directory\n",
    "        self.labels = torch.Tensor(df['newid'].values).long()\n",
    "        self.imgs = torch.cat([ self.transforms(self.resize_img(Image.open(f'{DIRECTROY}/{self.directory}/{x}')).convert('RGB')).half().reshape(1,3,IMG_SIZE,IMG_SIZE) for x in tqdm(df['name'].values)])\n",
    "        self.tokenized = self.tokenizer(df['label'].tolist(), padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        self.input_ids = self.tokenized['input_ids']\n",
    "        self.attention_mask = self.tokenized['attention_mask']\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.imgs[idx]\n",
    "        label = self.labels[idx]\n",
    "        input_ids = self.input_ids[idx]\n",
    "        attention_mask = self.attention_mask[idx]\n",
    "        return img, label, input_ids, attention_mask\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\functional.py:507: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ..\\aten\\src\\ATen\\native\\TensorShape.cpp:3550.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FasterViT(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Identity()\n",
       "    (conv_down): Sequential(\n",
       "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(64, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Conv2d(64, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (4): BatchNorm2d(96, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (levels): ModuleList(\n",
       "    (0): FasterViTLayer(\n",
       "      (blocks): ModuleList(\n",
       "        (0): ConvBlock(\n",
       "          (conv1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): GELU(approximate='none')\n",
       "          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (1): ConvBlock(\n",
       "          (conv1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): GELU(approximate='none')\n",
       "          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (drop_path): DropPath(drop_prob=0.011)\n",
       "        )\n",
       "        (2): ConvBlock(\n",
       "          (conv1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): GELU(approximate='none')\n",
       "          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (drop_path): DropPath(drop_prob=0.022)\n",
       "        )\n",
       "      )\n",
       "      (downsample): Downsample(\n",
       "        (norm): LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)\n",
       "        (reduction): Sequential(\n",
       "          (0): Conv2d(96, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): FasterViTLayer(\n",
       "      (blocks): ModuleList(\n",
       "        (0): ConvBlock(\n",
       "          (conv1): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): GELU(approximate='none')\n",
       "          (conv2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (drop_path): DropPath(drop_prob=0.033)\n",
       "        )\n",
       "        (1): ConvBlock(\n",
       "          (conv1): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): GELU(approximate='none')\n",
       "          (conv2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (drop_path): DropPath(drop_prob=0.044)\n",
       "        )\n",
       "        (2): ConvBlock(\n",
       "          (conv1): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): GELU(approximate='none')\n",
       "          (conv2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (drop_path): DropPath(drop_prob=0.056)\n",
       "        )\n",
       "      )\n",
       "      (downsample): Downsample(\n",
       "        (norm): LayerNorm2d((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (reduction): Sequential(\n",
       "          (0): Conv2d(192, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): FasterViTLayer(\n",
       "      (blocks): ModuleList(\n",
       "        (0): HAT(\n",
       "          (pos_embed): PosEmbMLPSwinv1D(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU()\n",
       "              (2): Linear(in_features=512, out_features=384, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (pos_emb_funct): PosEmbMLPSwinv2D(\n",
       "              (cpb_mlp): Sequential(\n",
       "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "                (1): ReLU(inplace=True)\n",
       "                (2): Linear(in_features=512, out_features=8, bias=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.067)\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (hat_norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (hat_norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (hat_attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (pos_emb_funct): PosEmbMLPSwinv2D(\n",
       "              (cpb_mlp): Sequential(\n",
       "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "                (1): ReLU(inplace=True)\n",
       "                (2): Linear(in_features=512, out_features=8, bias=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (hat_mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (hat_drop_path): DropPath(drop_prob=0.067)\n",
       "          (hat_pos_embed): PosEmbMLPSwinv1D(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU()\n",
       "              (2): Linear(in_features=512, out_features=384, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (upsampler): Upsample(size=7, mode='nearest')\n",
       "        )\n",
       "        (1): HAT(\n",
       "          (pos_embed): PosEmbMLPSwinv1D(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU()\n",
       "              (2): Linear(in_features=512, out_features=384, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (pos_emb_funct): PosEmbMLPSwinv2D(\n",
       "              (cpb_mlp): Sequential(\n",
       "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "                (1): ReLU(inplace=True)\n",
       "                (2): Linear(in_features=512, out_features=8, bias=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.078)\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (hat_norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (hat_norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (hat_attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (pos_emb_funct): PosEmbMLPSwinv2D(\n",
       "              (cpb_mlp): Sequential(\n",
       "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "                (1): ReLU(inplace=True)\n",
       "                (2): Linear(in_features=512, out_features=8, bias=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (hat_mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (hat_drop_path): DropPath(drop_prob=0.078)\n",
       "          (hat_pos_embed): PosEmbMLPSwinv1D(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU()\n",
       "              (2): Linear(in_features=512, out_features=384, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (upsampler): Upsample(size=7, mode='nearest')\n",
       "        )\n",
       "        (2): HAT(\n",
       "          (pos_embed): PosEmbMLPSwinv1D(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU()\n",
       "              (2): Linear(in_features=512, out_features=384, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (pos_emb_funct): PosEmbMLPSwinv2D(\n",
       "              (cpb_mlp): Sequential(\n",
       "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "                (1): ReLU(inplace=True)\n",
       "                (2): Linear(in_features=512, out_features=8, bias=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.089)\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (hat_norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (hat_norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (hat_attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (pos_emb_funct): PosEmbMLPSwinv2D(\n",
       "              (cpb_mlp): Sequential(\n",
       "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "                (1): ReLU(inplace=True)\n",
       "                (2): Linear(in_features=512, out_features=8, bias=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (hat_mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (hat_drop_path): DropPath(drop_prob=0.089)\n",
       "          (hat_pos_embed): PosEmbMLPSwinv1D(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU()\n",
       "              (2): Linear(in_features=512, out_features=384, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (upsampler): Upsample(size=7, mode='nearest')\n",
       "        )\n",
       "        (3): HAT(\n",
       "          (pos_embed): PosEmbMLPSwinv1D(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU()\n",
       "              (2): Linear(in_features=512, out_features=384, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (pos_emb_funct): PosEmbMLPSwinv2D(\n",
       "              (cpb_mlp): Sequential(\n",
       "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "                (1): ReLU(inplace=True)\n",
       "                (2): Linear(in_features=512, out_features=8, bias=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.100)\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (hat_norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (hat_norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (hat_attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (pos_emb_funct): PosEmbMLPSwinv2D(\n",
       "              (cpb_mlp): Sequential(\n",
       "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "                (1): ReLU(inplace=True)\n",
       "                (2): Linear(in_features=512, out_features=8, bias=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (hat_mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (hat_drop_path): DropPath(drop_prob=0.100)\n",
       "          (hat_pos_embed): PosEmbMLPSwinv1D(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU()\n",
       "              (2): Linear(in_features=512, out_features=384, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (upsampler): Upsample(size=7, mode='nearest')\n",
       "        )\n",
       "        (4): HAT(\n",
       "          (pos_embed): PosEmbMLPSwinv1D(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU()\n",
       "              (2): Linear(in_features=512, out_features=384, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (pos_emb_funct): PosEmbMLPSwinv2D(\n",
       "              (cpb_mlp): Sequential(\n",
       "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "                (1): ReLU(inplace=True)\n",
       "                (2): Linear(in_features=512, out_features=8, bias=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.111)\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (hat_norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (hat_norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (hat_attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (pos_emb_funct): PosEmbMLPSwinv2D(\n",
       "              (cpb_mlp): Sequential(\n",
       "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "                (1): ReLU(inplace=True)\n",
       "                (2): Linear(in_features=512, out_features=8, bias=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (hat_mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (hat_drop_path): DropPath(drop_prob=0.111)\n",
       "          (hat_pos_embed): PosEmbMLPSwinv1D(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU()\n",
       "              (2): Linear(in_features=512, out_features=384, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (upsampler): Upsample(size=7, mode='nearest')\n",
       "        )\n",
       "        (5): HAT(\n",
       "          (pos_embed): PosEmbMLPSwinv1D(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU()\n",
       "              (2): Linear(in_features=512, out_features=384, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (pos_emb_funct): PosEmbMLPSwinv2D(\n",
       "              (cpb_mlp): Sequential(\n",
       "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "                (1): ReLU(inplace=True)\n",
       "                (2): Linear(in_features=512, out_features=8, bias=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.122)\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (hat_norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (hat_norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (hat_attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (pos_emb_funct): PosEmbMLPSwinv2D(\n",
       "              (cpb_mlp): Sequential(\n",
       "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "                (1): ReLU(inplace=True)\n",
       "                (2): Linear(in_features=512, out_features=8, bias=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (hat_mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (hat_drop_path): DropPath(drop_prob=0.122)\n",
       "          (hat_pos_embed): PosEmbMLPSwinv1D(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU()\n",
       "              (2): Linear(in_features=512, out_features=384, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (upsampler): Upsample(size=7, mode='nearest')\n",
       "        )\n",
       "        (6): HAT(\n",
       "          (pos_embed): PosEmbMLPSwinv1D(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU()\n",
       "              (2): Linear(in_features=512, out_features=384, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (pos_emb_funct): PosEmbMLPSwinv2D(\n",
       "              (cpb_mlp): Sequential(\n",
       "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "                (1): ReLU(inplace=True)\n",
       "                (2): Linear(in_features=512, out_features=8, bias=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.133)\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (hat_norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (hat_norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (hat_attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (pos_emb_funct): PosEmbMLPSwinv2D(\n",
       "              (cpb_mlp): Sequential(\n",
       "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "                (1): ReLU(inplace=True)\n",
       "                (2): Linear(in_features=512, out_features=8, bias=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (hat_mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (hat_drop_path): DropPath(drop_prob=0.133)\n",
       "          (hat_pos_embed): PosEmbMLPSwinv1D(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU()\n",
       "              (2): Linear(in_features=512, out_features=384, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (upsampler): Upsample(size=7, mode='nearest')\n",
       "        )\n",
       "        (7): HAT(\n",
       "          (pos_embed): PosEmbMLPSwinv1D(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU()\n",
       "              (2): Linear(in_features=512, out_features=384, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (pos_emb_funct): PosEmbMLPSwinv2D(\n",
       "              (cpb_mlp): Sequential(\n",
       "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "                (1): ReLU(inplace=True)\n",
       "                (2): Linear(in_features=512, out_features=8, bias=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.144)\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (hat_norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (hat_norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (hat_attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (pos_emb_funct): PosEmbMLPSwinv2D(\n",
       "              (cpb_mlp): Sequential(\n",
       "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "                (1): ReLU(inplace=True)\n",
       "                (2): Linear(in_features=512, out_features=8, bias=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (hat_mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (hat_drop_path): DropPath(drop_prob=0.144)\n",
       "          (hat_pos_embed): PosEmbMLPSwinv1D(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU()\n",
       "              (2): Linear(in_features=512, out_features=384, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (upsampler): Upsample(size=7, mode='nearest')\n",
       "        )\n",
       "      )\n",
       "      (downsample): Downsample(\n",
       "        (norm): LayerNorm2d((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (reduction): Sequential(\n",
       "          (0): Conv2d(384, 768, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (global_tokenizer): TokenInitializer(\n",
       "        (pos_embed): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
       "        (to_global_feature): Sequential(\n",
       "          (pos): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
       "          (pool): AvgPool2d(kernel_size=5, stride=3, padding=0)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): FasterViTLayer(\n",
       "      (blocks): ModuleList(\n",
       "        (0): HAT(\n",
       "          (pos_embed): PosEmbMLPSwinv1D(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU()\n",
       "              (2): Linear(in_features=512, out_features=768, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (pos_emb_funct): PosEmbMLPSwinv2D(\n",
       "              (cpb_mlp): Sequential(\n",
       "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "                (1): ReLU(inplace=True)\n",
       "                (2): Linear(in_features=512, out_features=16, bias=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.156)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): HAT(\n",
       "          (pos_embed): PosEmbMLPSwinv1D(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU()\n",
       "              (2): Linear(in_features=512, out_features=768, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (pos_emb_funct): PosEmbMLPSwinv2D(\n",
       "              (cpb_mlp): Sequential(\n",
       "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "                (1): ReLU(inplace=True)\n",
       "                (2): Linear(in_features=512, out_features=16, bias=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.167)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): HAT(\n",
       "          (pos_embed): PosEmbMLPSwinv1D(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU()\n",
       "              (2): Linear(in_features=512, out_features=768, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (pos_emb_funct): PosEmbMLPSwinv2D(\n",
       "              (cpb_mlp): Sequential(\n",
       "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "                (1): ReLU(inplace=True)\n",
       "                (2): Linear(in_features=512, out_features=16, bias=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.178)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): HAT(\n",
       "          (pos_embed): PosEmbMLPSwinv1D(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU()\n",
       "              (2): Linear(in_features=512, out_features=768, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (pos_emb_funct): PosEmbMLPSwinv2D(\n",
       "              (cpb_mlp): Sequential(\n",
       "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "                (1): ReLU(inplace=True)\n",
       "                (2): Linear(in_features=512, out_features=16, bias=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.189)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): HAT(\n",
       "          (pos_embed): PosEmbMLPSwinv1D(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU()\n",
       "              (2): Linear(in_features=512, out_features=768, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (pos_emb_funct): PosEmbMLPSwinv2D(\n",
       "              (cpb_mlp): Sequential(\n",
       "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "                (1): ReLU(inplace=True)\n",
       "                (2): Linear(in_features=512, out_features=16, bias=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.200)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (head): Linear(in_features=768, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = 'faster_vit_2_224'\n",
    "model = create_model(model_name)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'faster_vit_2_224'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'models/download\\\\fastervit_2_224_1k.pth.tar'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "hf_hub_download(repo_id=\"ahatamiz/FasterViT\", filename='fastervit_2_224_1k.pth.tar', local_dir='models/download')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(f'{MODEL_PATH}/download/fastervit_2_224_1k.pth.tar'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.head = nn.Linear(model.head.in_features, num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of FasterViT(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Identity()\n",
       "    (conv_down): Sequential(\n",
       "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(64, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Conv2d(64, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (4): BatchNorm2d(96, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (levels): ModuleList(\n",
       "    (0): FasterViTLayer(\n",
       "      (blocks): ModuleList(\n",
       "        (0): ConvBlock(\n",
       "          (conv1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): GELU(approximate='none')\n",
       "          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (1): ConvBlock(\n",
       "          (conv1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): GELU(approximate='none')\n",
       "          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (drop_path): DropPath(drop_prob=0.011)\n",
       "        )\n",
       "        (2): ConvBlock(\n",
       "          (conv1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): GELU(approximate='none')\n",
       "          (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (drop_path): DropPath(drop_prob=0.022)\n",
       "        )\n",
       "      )\n",
       "      (downsample): Downsample(\n",
       "        (norm): LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)\n",
       "        (reduction): Sequential(\n",
       "          (0): Conv2d(96, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): FasterViTLayer(\n",
       "      (blocks): ModuleList(\n",
       "        (0): ConvBlock(\n",
       "          (conv1): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): GELU(approximate='none')\n",
       "          (conv2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (drop_path): DropPath(drop_prob=0.033)\n",
       "        )\n",
       "        (1): ConvBlock(\n",
       "          (conv1): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): GELU(approximate='none')\n",
       "          (conv2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (drop_path): DropPath(drop_prob=0.044)\n",
       "        )\n",
       "        (2): ConvBlock(\n",
       "          (conv1): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): GELU(approximate='none')\n",
       "          (conv2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (drop_path): DropPath(drop_prob=0.056)\n",
       "        )\n",
       "      )\n",
       "      (downsample): Downsample(\n",
       "        (norm): LayerNorm2d((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (reduction): Sequential(\n",
       "          (0): Conv2d(192, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): FasterViTLayer(\n",
       "      (blocks): ModuleList(\n",
       "        (0): HAT(\n",
       "          (pos_embed): PosEmbMLPSwinv1D(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU()\n",
       "              (2): Linear(in_features=512, out_features=384, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (pos_emb_funct): PosEmbMLPSwinv2D(\n",
       "              (cpb_mlp): Sequential(\n",
       "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "                (1): ReLU(inplace=True)\n",
       "                (2): Linear(in_features=512, out_features=8, bias=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.067)\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (hat_norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (hat_norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (hat_attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (pos_emb_funct): PosEmbMLPSwinv2D(\n",
       "              (cpb_mlp): Sequential(\n",
       "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "                (1): ReLU(inplace=True)\n",
       "                (2): Linear(in_features=512, out_features=8, bias=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (hat_mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (hat_drop_path): DropPath(drop_prob=0.067)\n",
       "          (hat_pos_embed): PosEmbMLPSwinv1D(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU()\n",
       "              (2): Linear(in_features=512, out_features=384, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (upsampler): Upsample(size=7, mode='nearest')\n",
       "        )\n",
       "        (1): HAT(\n",
       "          (pos_embed): PosEmbMLPSwinv1D(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU()\n",
       "              (2): Linear(in_features=512, out_features=384, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (pos_emb_funct): PosEmbMLPSwinv2D(\n",
       "              (cpb_mlp): Sequential(\n",
       "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "                (1): ReLU(inplace=True)\n",
       "                (2): Linear(in_features=512, out_features=8, bias=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.078)\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (hat_norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (hat_norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (hat_attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (pos_emb_funct): PosEmbMLPSwinv2D(\n",
       "              (cpb_mlp): Sequential(\n",
       "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "                (1): ReLU(inplace=True)\n",
       "                (2): Linear(in_features=512, out_features=8, bias=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (hat_mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (hat_drop_path): DropPath(drop_prob=0.078)\n",
       "          (hat_pos_embed): PosEmbMLPSwinv1D(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU()\n",
       "              (2): Linear(in_features=512, out_features=384, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (upsampler): Upsample(size=7, mode='nearest')\n",
       "        )\n",
       "        (2): HAT(\n",
       "          (pos_embed): PosEmbMLPSwinv1D(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU()\n",
       "              (2): Linear(in_features=512, out_features=384, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (pos_emb_funct): PosEmbMLPSwinv2D(\n",
       "              (cpb_mlp): Sequential(\n",
       "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "                (1): ReLU(inplace=True)\n",
       "                (2): Linear(in_features=512, out_features=8, bias=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.089)\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (hat_norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (hat_norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (hat_attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (pos_emb_funct): PosEmbMLPSwinv2D(\n",
       "              (cpb_mlp): Sequential(\n",
       "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "                (1): ReLU(inplace=True)\n",
       "                (2): Linear(in_features=512, out_features=8, bias=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (hat_mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (hat_drop_path): DropPath(drop_prob=0.089)\n",
       "          (hat_pos_embed): PosEmbMLPSwinv1D(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU()\n",
       "              (2): Linear(in_features=512, out_features=384, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (upsampler): Upsample(size=7, mode='nearest')\n",
       "        )\n",
       "        (3): HAT(\n",
       "          (pos_embed): PosEmbMLPSwinv1D(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU()\n",
       "              (2): Linear(in_features=512, out_features=384, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (pos_emb_funct): PosEmbMLPSwinv2D(\n",
       "              (cpb_mlp): Sequential(\n",
       "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "                (1): ReLU(inplace=True)\n",
       "                (2): Linear(in_features=512, out_features=8, bias=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.100)\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (hat_norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (hat_norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (hat_attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (pos_emb_funct): PosEmbMLPSwinv2D(\n",
       "              (cpb_mlp): Sequential(\n",
       "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "                (1): ReLU(inplace=True)\n",
       "                (2): Linear(in_features=512, out_features=8, bias=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (hat_mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (hat_drop_path): DropPath(drop_prob=0.100)\n",
       "          (hat_pos_embed): PosEmbMLPSwinv1D(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU()\n",
       "              (2): Linear(in_features=512, out_features=384, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (upsampler): Upsample(size=7, mode='nearest')\n",
       "        )\n",
       "        (4): HAT(\n",
       "          (pos_embed): PosEmbMLPSwinv1D(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU()\n",
       "              (2): Linear(in_features=512, out_features=384, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (pos_emb_funct): PosEmbMLPSwinv2D(\n",
       "              (cpb_mlp): Sequential(\n",
       "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "                (1): ReLU(inplace=True)\n",
       "                (2): Linear(in_features=512, out_features=8, bias=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.111)\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (hat_norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (hat_norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (hat_attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (pos_emb_funct): PosEmbMLPSwinv2D(\n",
       "              (cpb_mlp): Sequential(\n",
       "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "                (1): ReLU(inplace=True)\n",
       "                (2): Linear(in_features=512, out_features=8, bias=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (hat_mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (hat_drop_path): DropPath(drop_prob=0.111)\n",
       "          (hat_pos_embed): PosEmbMLPSwinv1D(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU()\n",
       "              (2): Linear(in_features=512, out_features=384, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (upsampler): Upsample(size=7, mode='nearest')\n",
       "        )\n",
       "        (5): HAT(\n",
       "          (pos_embed): PosEmbMLPSwinv1D(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU()\n",
       "              (2): Linear(in_features=512, out_features=384, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (pos_emb_funct): PosEmbMLPSwinv2D(\n",
       "              (cpb_mlp): Sequential(\n",
       "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "                (1): ReLU(inplace=True)\n",
       "                (2): Linear(in_features=512, out_features=8, bias=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.122)\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (hat_norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (hat_norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (hat_attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (pos_emb_funct): PosEmbMLPSwinv2D(\n",
       "              (cpb_mlp): Sequential(\n",
       "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "                (1): ReLU(inplace=True)\n",
       "                (2): Linear(in_features=512, out_features=8, bias=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (hat_mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (hat_drop_path): DropPath(drop_prob=0.122)\n",
       "          (hat_pos_embed): PosEmbMLPSwinv1D(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU()\n",
       "              (2): Linear(in_features=512, out_features=384, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (upsampler): Upsample(size=7, mode='nearest')\n",
       "        )\n",
       "        (6): HAT(\n",
       "          (pos_embed): PosEmbMLPSwinv1D(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU()\n",
       "              (2): Linear(in_features=512, out_features=384, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (pos_emb_funct): PosEmbMLPSwinv2D(\n",
       "              (cpb_mlp): Sequential(\n",
       "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "                (1): ReLU(inplace=True)\n",
       "                (2): Linear(in_features=512, out_features=8, bias=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.133)\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (hat_norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (hat_norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (hat_attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (pos_emb_funct): PosEmbMLPSwinv2D(\n",
       "              (cpb_mlp): Sequential(\n",
       "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "                (1): ReLU(inplace=True)\n",
       "                (2): Linear(in_features=512, out_features=8, bias=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (hat_mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (hat_drop_path): DropPath(drop_prob=0.133)\n",
       "          (hat_pos_embed): PosEmbMLPSwinv1D(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU()\n",
       "              (2): Linear(in_features=512, out_features=384, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (upsampler): Upsample(size=7, mode='nearest')\n",
       "        )\n",
       "        (7): HAT(\n",
       "          (pos_embed): PosEmbMLPSwinv1D(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU()\n",
       "              (2): Linear(in_features=512, out_features=384, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (pos_emb_funct): PosEmbMLPSwinv2D(\n",
       "              (cpb_mlp): Sequential(\n",
       "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "                (1): ReLU(inplace=True)\n",
       "                (2): Linear(in_features=512, out_features=8, bias=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.144)\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (hat_norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (hat_norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (hat_attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (pos_emb_funct): PosEmbMLPSwinv2D(\n",
       "              (cpb_mlp): Sequential(\n",
       "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "                (1): ReLU(inplace=True)\n",
       "                (2): Linear(in_features=512, out_features=8, bias=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (hat_mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (hat_drop_path): DropPath(drop_prob=0.144)\n",
       "          (hat_pos_embed): PosEmbMLPSwinv1D(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU()\n",
       "              (2): Linear(in_features=512, out_features=384, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (upsampler): Upsample(size=7, mode='nearest')\n",
       "        )\n",
       "      )\n",
       "      (downsample): Downsample(\n",
       "        (norm): LayerNorm2d((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (reduction): Sequential(\n",
       "          (0): Conv2d(384, 768, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (global_tokenizer): TokenInitializer(\n",
       "        (pos_embed): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
       "        (to_global_feature): Sequential(\n",
       "          (pos): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
       "          (pool): AvgPool2d(kernel_size=5, stride=3, padding=0)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): FasterViTLayer(\n",
       "      (blocks): ModuleList(\n",
       "        (0): HAT(\n",
       "          (pos_embed): PosEmbMLPSwinv1D(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU()\n",
       "              (2): Linear(in_features=512, out_features=768, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (pos_emb_funct): PosEmbMLPSwinv2D(\n",
       "              (cpb_mlp): Sequential(\n",
       "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "                (1): ReLU(inplace=True)\n",
       "                (2): Linear(in_features=512, out_features=16, bias=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.156)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): HAT(\n",
       "          (pos_embed): PosEmbMLPSwinv1D(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU()\n",
       "              (2): Linear(in_features=512, out_features=768, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (pos_emb_funct): PosEmbMLPSwinv2D(\n",
       "              (cpb_mlp): Sequential(\n",
       "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "                (1): ReLU(inplace=True)\n",
       "                (2): Linear(in_features=512, out_features=16, bias=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.167)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): HAT(\n",
       "          (pos_embed): PosEmbMLPSwinv1D(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU()\n",
       "              (2): Linear(in_features=512, out_features=768, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (pos_emb_funct): PosEmbMLPSwinv2D(\n",
       "              (cpb_mlp): Sequential(\n",
       "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "                (1): ReLU(inplace=True)\n",
       "                (2): Linear(in_features=512, out_features=16, bias=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.178)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): HAT(\n",
       "          (pos_embed): PosEmbMLPSwinv1D(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU()\n",
       "              (2): Linear(in_features=512, out_features=768, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (pos_emb_funct): PosEmbMLPSwinv2D(\n",
       "              (cpb_mlp): Sequential(\n",
       "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "                (1): ReLU(inplace=True)\n",
       "                (2): Linear(in_features=512, out_features=16, bias=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.189)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): HAT(\n",
       "          (pos_embed): PosEmbMLPSwinv1D(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU()\n",
       "              (2): Linear(in_features=512, out_features=768, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (pos_emb_funct): PosEmbMLPSwinv2D(\n",
       "              (cpb_mlp): Sequential(\n",
       "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "                (1): ReLU(inplace=True)\n",
       "                (2): Linear(in_features=512, out_features=16, bias=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.200)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (head): Linear(in_features=768, out_features=649, bias=True)\n",
       ")>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = LR)\n",
    "scheduler = optim.lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.1, total_iters=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torch.load(f'{DIRECTROY}/train_dataset/train_dataset_reduced_all.pth')\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 914/914 [02:53<00:00,  5.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 0.012702874470382009\n",
      "Evaluating epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 190/190 [00:09<00:00, 19.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 0.01709205884450888\n",
      "Accuracy: 0.8486344192168477\n",
      "F1 Score Weighted: 0.838418301844026\n",
      "F1 Score Macro: 0.8283943653500483\n",
      "Training epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 914/914 [02:53<00:00,  5.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100, Loss: 0.007526192254356364\n",
      "Evaluating epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 190/190 [00:10<00:00, 18.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100, Loss: 0.015395997421875444\n",
      "Accuracy: 0.8624547548535703\n",
      "F1 Score Weighted: 0.8539864546343388\n",
      "F1 Score Macro: 0.8476788396999014\n",
      "Training epoch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 914/914 [02:52<00:00,  5.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/100, Loss: 0.004786862650183554\n",
      "Evaluating epoch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 190/190 [00:10<00:00, 18.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/100, Loss: 0.01469031646761917\n",
      "Accuracy: 0.8731490621915103\n",
      "F1 Score Weighted: 0.8676769342269359\n",
      "F1 Score Macro: 0.8602605576820445\n",
      "Training epoch: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 914/914 [02:53<00:00,  5.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/100, Loss: 0.0035698023168563445\n",
      "Evaluating epoch: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 190/190 [00:09<00:00, 19.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/100, Loss: 0.01429912959099522\n",
      "Accuracy: 0.8821980914774596\n",
      "F1 Score Weighted: 0.8793292082544544\n",
      "F1 Score Macro: 0.8742294862418476\n",
      "Training epoch: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 914/914 [02:54<00:00,  5.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/100, Loss: 0.002442129030984857\n",
      "Evaluating epoch: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 190/190 [00:10<00:00, 18.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/100, Loss: 0.015483331427103146\n",
      "Accuracy: 0.8743007568279039\n",
      "F1 Score Weighted: 0.8707719383490601\n",
      "F1 Score Macro: 0.8655070236979799\n",
      "Training epoch: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 914/914 [02:53<00:00,  5.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/100, Loss: 0.002110354770715725\n",
      "Evaluating epoch: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 190/190 [00:10<00:00, 18.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/100, Loss: 0.014776523946868712\n",
      "Accuracy: 0.8769332017110891\n",
      "F1 Score Weighted: 0.8737970706451135\n",
      "F1 Score Macro: 0.8690520124755792\n",
      "Training epoch: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 914/914 [02:53<00:00,  5.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/100, Loss: 0.001719741793041418\n",
      "Evaluating epoch: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 190/190 [00:09<00:00, 19.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/100, Loss: 0.01649661590205766\n",
      "Accuracy: 0.8693649226719316\n",
      "F1 Score Weighted: 0.8673617763649845\n",
      "F1 Score Macro: 0.863625714207672\n",
      "Training epoch: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 914/914 [02:54<00:00,  5.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/100, Loss: 0.0013506729480466284\n",
      "Evaluating epoch: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 190/190 [00:09<00:00, 19.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/100, Loss: 0.015564015693380305\n",
      "Accuracy: 0.8823626192826588\n",
      "F1 Score Weighted: 0.8797608421060014\n",
      "F1 Score Macro: 0.8756540138770176\n",
      "Training epoch: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 914/914 [02:53<00:00,  5.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/100, Loss: 0.0011123626437852848\n",
      "Evaluating epoch: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 190/190 [00:10<00:00, 18.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/100, Loss: 0.016535309732681084\n",
      "Accuracy: 0.870352089503126\n",
      "F1 Score Weighted: 0.8691346125449892\n",
      "F1 Score Macro: 0.8661875035399134\n",
      "Training epoch: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 914/914 [02:54<00:00,  5.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100, Loss: 0.0012354266004487135\n",
      "Evaluating epoch: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 190/190 [00:09<00:00, 19.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100, Loss: 0.016366731286276893\n",
      "Accuracy: 0.87857847976308\n",
      "F1 Score Weighted: 0.8765954926844829\n",
      "F1 Score Macro: 0.8739651039511058\n",
      "Training epoch: 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 914/914 [02:50<00:00,  5.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/100, Loss: 0.0008732294518083617\n",
      "Evaluating epoch: 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 190/190 [00:10<00:00, 18.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/100, Loss: 0.016625458275327444\n",
      "Accuracy: 0.8818690358670616\n",
      "F1 Score Weighted: 0.8805085564850711\n",
      "F1 Score Macro: 0.8773812722462595\n",
      "Training epoch: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 914/914 [02:54<00:00,  5.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/100, Loss: 0.0009732399032100028\n",
      "Evaluating epoch: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 190/190 [00:10<00:00, 18.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/100, Loss: 0.01711531950304283\n",
      "Accuracy: 0.87857847976308\n",
      "F1 Score Weighted: 0.8772351175693937\n",
      "F1 Score Macro: 0.875272409406591\n",
      "Training epoch: 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 914/914 [02:55<00:00,  5.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/100, Loss: 0.0007087814300873122\n",
      "Evaluating epoch: 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 190/190 [00:10<00:00, 18.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/100, Loss: 0.016787247388360408\n",
      "Accuracy: 0.8854886475814413\n",
      "F1 Score Weighted: 0.8840676629390143\n",
      "F1 Score Macro: 0.8783949965944572\n",
      "Training epoch: 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 914/914 [02:56<00:00,  5.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/100, Loss: 0.0008321502416905202\n",
      "Evaluating epoch: 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 190/190 [00:10<00:00, 18.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/100, Loss: 0.01747257455812008\n",
      "Accuracy: 0.8784139519578809\n",
      "F1 Score Weighted: 0.8761649179587658\n",
      "F1 Score Macro: 0.8725562685027308\n",
      "Training epoch: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 101/914 [00:19<02:36,  5.20it/s]"
     ]
    }
   ],
   "source": [
    "max_accuracy = 0.0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    # Training loop\n",
    "    print('Training epoch:', epoch+1)\n",
    "    len_train = 0\n",
    " \n",
    "    \n",
    "\n",
    "    \n",
    "    for inputs, labels, input_ids, attention_mask in tqdm(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        inputs = inputs.to(device).type(torch.cuda.FloatTensor)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "    len_train += len(train_dataset)\n",
    "   \n",
    "        \n",
    "    scheduler.step()    \n",
    "    train_loss/=len_train\n",
    "    print(f'Epoch {epoch+1}/{EPOCHS}, Loss: {train_loss}')\n",
    "    \n",
    "    eval_loss = 0.0\n",
    "    model.eval()\n",
    "    \n",
    "    true_labels = []\n",
    "    pred_labels = []\n",
    "    \n",
    "    print('Evaluating epoch:', epoch+1)\n",
    "    with torch.no_grad():\n",
    "        len_test = 0\n",
    "        \n",
    "        test_dataset = torch.load(f'{DIRECTROY}/test_public_dataset/test_public_reduced_dataset_0.pth')\n",
    "        test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "          \n",
    "\n",
    "        for inputs, labels, input_ids, attention_mask in tqdm(test_dataloader):\n",
    "            inputs = inputs.to(device).type(torch.cuda.FloatTensor)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            loss = criterion(outputs, labels).to(device)\n",
    "            eval_loss += loss.item()\n",
    "            \n",
    "            outputs = torch.argmax(outputs, 1).flatten().cpu().numpy()\n",
    "            labels = labels.flatten().cpu().numpy()\n",
    "            \n",
    "            true_labels.extend(labels)\n",
    "            pred_labels.extend(outputs)\n",
    "        \n",
    "        len_test += len(test_dataset)\n",
    "        \n",
    "        \n",
    "    print(f'Epoch {epoch+1}/{EPOCHS}, Loss: {eval_loss/len_test}')\n",
    "    print(f'Accuracy: {accuracy_score(true_labels, pred_labels)}')\n",
    "    print(f'F1 Score Weighted: {f1_score(true_labels, pred_labels, average=\"weighted\")}')\n",
    "    print(f'F1 Score Macro: {f1_score(true_labels, pred_labels, average=\"macro\")}')\n",
    "    if accuracy_score(true_labels, pred_labels) > max_accuracy:\n",
    "        max_accuracy = accuracy_score(true_labels, pred_labels)\n",
    "        torch.save(model.state_dict(), f'{MODEL_PATH}/faster_vit_reduced_model_{epoch+1}.pth')\n",
    "        torch.save(optimizer.state_dict(), f'{MODEL_PATH}/optimizer/faster_vit_reduced_optimizer_{epoch+1}.pth')\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
