{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPTokenizerFast, CLIPModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.transforms import Compose, ToTensor,  Resize, Normalize\n",
    "from PIL import Image, ImageDraw\n",
    "from torch.utils.data import DataLoader, Dataset, BatchSampler\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetune CLIP for image classification\n",
    "The idea is to re-train CLIP by matching images with their descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIRECTROY = 'data'\n",
    "MODEL_PATH = 'models'\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 100\n",
    "LR = 0.0001\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(f'{DIRECTROY}/reduced_train.csv') \n",
    "df_test = pd.read_csv(f'{DIRECTROY}/reduced_test.csv') \n",
    "num_classes = len(df_train['class'].unique())\n",
    "classes = df_train['class'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class2label = {c:l for c, l in zip(df_train['newid'], df_train['label'])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_class2label = dict(sorted(class2label.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "for k, v in sorted_class2label.items():\n",
    "    labels.append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_public = df_test[df_test['Usage'] == 'Public']\n",
    "df_test_private = df_test[df_test['Usage'] == 'Private']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_transforms = Compose([\n",
    "    Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    ToTensor(), \n",
    "    Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df, transforms, directory):\n",
    "        self.tokenizer =  CLIPTokenizerFast.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
    "        self.df = df\n",
    "        self.transforms = transforms\n",
    "        self.directory = directory\n",
    "        self.labels = torch.Tensor(df['newid'].values).long()\n",
    "        self.imgs = torch.cat([ self.transforms(self.resize_img(Image.open(f'{DIRECTROY}/{self.directory}/{x}')).convert('RGB')).half().reshape(1,3,IMG_SIZE,IMG_SIZE) for x in tqdm(df['name'].values)])\n",
    "        self.tokenized = self.tokenizer(df['label'].tolist(), padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        self.input_ids = self.tokenized['input_ids']\n",
    "        self.attention_mask = self.tokenized['attention_mask']\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.imgs[idx]\n",
    "        label = self.labels[idx]\n",
    "        input_ids = self.input_ids[idx]\n",
    "        attention_mask = self.attention_mask[idx]\n",
    "        return img, label, input_ids, attention_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CustomSampler for Dataloader\n",
    "\n",
    "CLIP requires during each iteration, the classes must me different from each others. This custom sampler will always make sure that think happen.\n",
    "\n",
    "The label just ensure that it does not larger than the dataloader length\n",
    "\n",
    "It basicly place list of labels in random position, concatenate values of those position in a vector, adding the 0s and warp it in form of matrix (bs, x). With batchs that contain 0s (happen less than batch size), it will randomly filled with other labels not in the batchs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSampler(BatchSampler):\n",
    "    def __init__(self, labels, n_samples, n_classes):\n",
    "        \n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
    "        self.labels = torch.IntTensor(labels.values).to(self.device)\n",
    "        self.n_samples = n_samples\n",
    "        self.n_classes = n_classes\n",
    "        self.num_labels = len(self.labels.unique())\n",
    "        self.labels_idx = dict()\n",
    "        for i in range(self.num_labels):\n",
    "            self.labels_idx[i] = torch.where(self.labels == i)[0]\n",
    "            \n",
    "        self.bs = n_samples*n_classes\n",
    "        \n",
    "        self.extra = (self.bs - len(self.labels)%self.bs)%self.bs\n",
    "        \n",
    "    @staticmethod\n",
    "    def random_mix(ts):\n",
    "        order = torch.randperm(len(ts))\n",
    "        return ts[order] \n",
    "\n",
    "    def __iter__(self):\n",
    "        order = np.random.choice(self.num_labels, self.num_labels, replace=False)\n",
    "        idxs = torch.cat([self.random_mix(self.labels_idx[l]) for l in order]).to(self.device) + 1\n",
    "        idxs = torch.cat([idxs, torch.zeros(self.extra).long().to(self.device)]).to(self.device)\n",
    "        idxs = idxs.view(self.bs,len(idxs)//self.bs).T\n",
    "        \n",
    "        for bs in idxs:\n",
    "            get = torch.nonzero(bs)\n",
    "            re = bs[get].squeeze() - 1\n",
    "            re = re.to(self.device)\n",
    "            count = 0\n",
    "            i = 0\n",
    "            # print(self.random_mix(self.labels_idx[order[i]])[count])\n",
    "            if len(re) < self.bs:\n",
    "                if count>len(self.labels_idx[order[i]]):\n",
    "                    count = 0\n",
    "                    i += 1\n",
    "                lucky = torch.ones(1, device=self.device)*self.random_mix(self.labels_idx[order[i]])[count]\n",
    "                re = torch.cat([re, lucky.long().to(self.device)])\n",
    "                count += 1\n",
    "            # print(re.tolist())\n",
    "     \n",
    "            yield re\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the dataset from drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torch.load(f'{DIRECTROY}/train_dataset/train_dataset_reduced_all.pth')\n",
    "train_dataloader = DataLoader(train_dataset, batch_sampler=CustomSampler(df_train['newid'],  BATCH_SIZE,1))\n",
    "test_dataset = torch.load(f'{DIRECTROY}/test_public_dataset/test_public_reduced_dataset_0.pth')\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
    "model = model.to(device)\n",
    "tokenizer = CLIPTokenizerFast.from_pretrained(\"openai/clip-vit-base-patch16\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passing every prompts during testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_prompts = tokenizer([f'a photo of {x}' for x in labels], return_tensors=\"pt\", padding=True, truncation=True)\n",
    "all_prompts['input_ids'] = all_prompts['input_ids'].to(device)\n",
    "all_prompts['attention_mask'] = all_prompts['attention_mask'].to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_img = nn.CrossEntropyLoss()\n",
    "loss_txt = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = LR)\n",
    "scheduler = optim.lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.1, total_iters=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = torch.load(f'{DIRECTROY}/test_public_dataset/test_public_reduced_dataset_0.pth')\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1137/1137 [05:19<00:00,  3.56it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 34\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m train_dataset\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m train_dataloader\n\u001b[1;32m---> 34\u001b[0m     len_train \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[43mtrain_dataset\u001b[49m)\n\u001b[0;32m     36\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep()    \n\u001b[0;32m     37\u001b[0m train_loss\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39mlen_train\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "max_accuracy = 0\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    # Training loop\n",
    "    print('Training epoch:', epoch+1)\n",
    "    len_train = 0\n",
    "    \n",
    "    for i in range(2):\n",
    "        train_dataset = torch.load(f'{DIRECTROY}/train_dataset/train_dataset_reduced_aug_{i}.pth')\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "        for inputs, labels, input_ids, attention_mask  in tqdm(train_dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            \n",
    "            logits = model(pixel_values=inputs, input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits_per_image = logits.logits_per_image\n",
    "            logits_per_text = logits.logits_per_text\n",
    "            \n",
    "            ground_truth = torch.arange(BATCH_SIZE).to(device)\n",
    "            \n",
    "            loss = (loss_img(logits_per_image,ground_truth) + loss_txt(logits_per_text,ground_truth))/2\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        len_train += len(train_dataset)\n",
    "        del train_dataset\n",
    "        del train_dataloader\n",
    "\n",
    "    scheduler.step()    \n",
    "    train_loss/=len_train\n",
    "    print(f'Epoch {epoch+1}/{EPOCHS}, Loss: {train_loss}')\n",
    "    \n",
    "    eval_loss = 0.0\n",
    "    model.eval()\n",
    "    \n",
    "    true_labels = []\n",
    "    pred_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        len_test = 0\n",
    "\n",
    "        \n",
    "        for inputs, labels, input_ids, attention_mask  in tqdm(test_dataloader):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            \n",
    "            logits = model(pixel_values=inputs, input_ids=all_prompts['input_ids'], attention_mask=all_prompts['attention_mask'])\n",
    "            logits_per_image = logits.logits_per_image\n",
    "            \n",
    "            outputs = torch.argmax(logits_per_image, 1).flatten().cpu().numpy()\n",
    "            labels = labels.flatten().cpu().numpy()\n",
    "            \n",
    "            \n",
    "            true_labels.extend(labels)\n",
    "            pred_labels.extend(outputs)\n",
    "        len_test += len(test_dataset)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{EPOCHS},')\n",
    "        print(f'Accuracy: {accuracy_score(true_labels, pred_labels)}')\n",
    "        print(f'F1 Score Weighted: {f1_score(true_labels, pred_labels, average=\"weighted\")}')\n",
    "        print(f'F1 Score Macro: {f1_score(true_labels, pred_labels, average=\"macro\")}')\n",
    "        if accuracy_score(true_labels, pred_labels) > max_accuracy:\n",
    "            max_accuracy = accuracy_score(true_labels, pred_labels)\n",
    "            torch.save(model, f'{MODEL_PATH}/model_clip_{epoch+1}.pth')\n",
    "            torch.save(optimizer, f'{MODEL_PATH}/optimizer/optimizer_clip_{epoch+1}.pth')\n",
    "            \n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
